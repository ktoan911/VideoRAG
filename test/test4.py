# file: requirements.txt
# --------------------------------------------------
# C√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ ch·∫°y m√£ ngu·ªìn MVP.
# C√†i ƒë·∫∑t b·∫±ng l·ªánh: pip install -r requirements.txt
# --------------------------------------------------

streamlit
pandas
numpy
torch
torchvision
Pillow
# Th∆∞ vi·ªán clip c·ªßa OpenAI ƒë·ªÉ t·∫°o embedding
# C√†i ƒë·∫∑t t·ª´ github: pip install git+https://github.com/openai/CLIP.git
clip @ git+https://github.com/openai/CLIP.git

```

```python
# file: ingest_data.py
# --------------------------------------------------
# GIAI ƒêO·∫†N 1: MVP - X·ª¨ L√ù V√Ä L·∫¨P CH·ªà M·ª§C D·ªÆ LI·ªÜU
# --------------------------------------------------
# Script n√†y m√¥ ph·ªèng qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o (ingestion).
# N√≥ ƒë·ªçc file JSON, t·∫°o embedding cho vƒÉn b·∫£n v√† h√¨nh ·∫£nh b·∫±ng m√¥ h√¨nh CLIP,
# v√† l∆∞u tr·ªØ si√™u d·ªØ li·ªáu c√πng v·ªõi c√°c vector embedding ƒë·ªÉ chu·∫©n b·ªã cho vi·ªác truy xu·∫•t.
# Trong m·ªôt h·ªá th·ªëng th·ª±c t·∫ø, d·ªØ li·ªáu n√†y s·∫Ω ƒë∆∞·ª£c ƒë·∫©y v√†o Elasticsearch v√† Milvus.[1]
# --------------------------------------------------

import json
import torch
import clip
from PIL import Image
import numpy as np
import pandas as pd
import os

# --- C·∫•u h√¨nh ---
# ƒê·∫£m b·∫£o b·∫°n ƒë√£ t·∫£i m√¥ h√¨nh CLIP. L·∫ßn ch·∫°y ƒë·∫ßu ti√™n s·∫Ω t·ª± ƒë·ªông t·∫£i v·ªÅ.
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL, PREPROCESS = clip.load("ViT-B/32", device=DEVICE)
print(f"S·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}")

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn d·ªØ li·ªáu ƒë·∫ßu v√†o
INPUT_JSON_PATH = 'data.json' # File JSON b·∫°n cung c·∫•p
KEYFRAME_DIR = 'keyframes/'   # Th∆∞ m·ª•c ch·ª©a c√°c keyframe (v√≠ d·ª•: 'Keyframe-003-.jpg')

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn c√°c file ƒë·∫ßu ra (m√¥ ph·ªèng c∆° s·ªü d·ªØ li·ªáu)
METADATA_OUTPUT_PATH = 'db_metadata.csv'
EMBEDDINGS_OUTPUT_PATH = 'db_embeddings.npz'

def create_super_document(segment):
    """
    K·∫øt h·ª£p t·∫•t c·∫£ c√°c tr∆∞·ªùng vƒÉn b·∫£n th√†nh m·ªôt t√†i li·ªáu duy nh·∫•t ƒë·ªÉ t·∫°o embedding.
    ƒêi·ªÅu n√†y gi√∫p n·∫Øm b·∫Øt to√†n b·ªô ng·ªØ nghƒ©a vƒÉn b·∫£n c·ªßa ƒëo·∫°n video.
    """
    texts = [
        segment.get('title', ''),
        segment.get('summary', ''),
    ]
    # Th√™m n·ªôi dung t·ª´ caption
    if 'caption' in segment and 'content' in segment['caption']:
        for cap in segment['caption']['content']:
            texts.append(cap.get('caption', ''))
    # Th√™m n·ªôi dung t·ª´ transcription
    if 'transcription' in segment and 'content' in segment['transcription']:
        for trans in segment['transcription']['content']:
            texts.append(trans.get('content', ''))
    
    return ". ".join(filter(None, texts))

def generate_embeddings_for_data(data):
    """
    T·∫°o v√† l∆∞u tr·ªØ embeddings cho to√†n b·ªô t·∫≠p d·ªØ li·ªáu.
    """
    metadata_list =
    text_embeddings =
    image_embeddings =

    # Gi·∫£ s·ª≠ d·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt danh s√°ch c√°c dictionary
    if not isinstance(data, list):
        data = [data]

    for i, segment in enumerate(data):
        # --- X·ª≠ l√Ω VƒÉn b·∫£n ---
        super_doc = create_super_document(segment)
        if super_doc:
            text_tokens = clip.tokenize([super_doc], truncate=True).to(DEVICE)
            with torch.no_grad():
                text_features = MODEL.encode_text(text_tokens)
                text_embeddings.append(text_features.cpu().numpy())
        else:
            # N·∫øu kh√¥ng c√≥ vƒÉn b·∫£n, s·ª≠ d·ª•ng vector zero
            text_embeddings.append(np.zeros((1, 512)))

        # --- X·ª≠ l√Ω H√¨nh ·∫£nh ---
        keyframe_filename = segment.get('keyframe')
        if keyframe_filename:
            image_path = os.path.join(KEYFRAME_DIR, keyframe_filename)
            if os.path.exists(image_path):
                image = PREPROCESS(Image.open(image_path)).unsqueeze(0).to(DEVICE)
                with torch.no_grad():
                    image_features = MODEL.encode_image(image)
                    image_embeddings.append(image_features.cpu().numpy())
            else:
                print(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file keyframe '{image_path}'")
                image_embeddings.append(np.zeros((1, 512)))
        else:
            image_embeddings.append(np.zeros((1, 512)))
            
        # --- L∆∞u tr·ªØ Si√™u d·ªØ li·ªáu ---
        # Gi·ªØ l·∫°i c√°c th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£
        metadata = {
            'id': i,
            'title': segment.get('title', ''),
            'summary': segment.get('summary', ''),
            'keyframe': segment.get('keyframe', ''),
            'start_time': segment.get('start_time', ''),
            'end_time': segment.get('end_time', ''),
        }
        metadata_list.append(metadata)

    # Chuy·ªÉn ƒë·ªïi danh s√°ch th√†nh c√°c m·∫£ng numpy
    text_embeddings_np = np.vstack(text_embeddings)
    image_embeddings_np = np.vstack(image_embeddings)

    # Chu·∫©n h√≥a embeddings (quan tr·ªçng cho vi·ªác t√≠nh to√°n cosine similarity)
    text_embeddings_np /= np.linalg.norm(text_embeddings_np, axis=1, keepdims=True)
    image_embeddings_np /= np.linalg.norm(image_embeddings_np, axis=1, keepdims=True)

    # L∆∞u si√™u d·ªØ li·ªáu v√† embeddings
    df_metadata = pd.DataFrame(metadata_list)
    df_metadata.to_csv(METADATA_OUTPUT_PATH, index=False)
    np.savez(EMBEDDINGS_OUTPUT_PATH, text=text_embeddings_np, image=image_embeddings_np)

    print(f"ƒê√£ x·ª≠ l√Ω {len(data)} ƒëo·∫°n video.")
    print(f"Si√™u d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {METADATA_OUTPUT_PATH}")
    print(f"Embeddings ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {EMBEDDINGS_OUTPUT_PATH}")

if __name__ == "__main__":
    # T·∫°o th∆∞ m·ª•c keyframes n·∫øu ch∆∞a c√≥
    if not os.path.exists(KEYFRAME_DIR):
        os.makedirs(KEYFRAME_DIR)
        print(f"ƒê√£ t·∫°o th∆∞ m·ª•c '{KEYFRAME_DIR}'. Vui l√≤ng ƒë·∫∑t c√°c file keyframe c·ªßa b·∫°n v√†o ƒë√¢y.")

    # ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON
    try:
        with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:
            video_data = json.load(f)
        generate_embeddings_for_data(video_data)
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{INPUT_JSON_PATH}'. Vui l√≤ng t·∫°o file n√†y v·ªõi d·ªØ li·ªáu c·ªßa b·∫°n.")
    except json.JSONDecodeError:
        print(f"L·ªói: File '{INPUT_JSON_PATH}' kh√¥ng ph·∫£i l√† m·ªôt file JSON h·ª£p l·ªá.")

```

```python
# file: retrieval_backend.py
# --------------------------------------------------
# GIAI ƒêO·∫†N 1: MVP - LOGIC TRUY XU·∫§T C·ªêT L√ïI
# --------------------------------------------------
# Script n√†y ch·ª©a logic backend ƒë·ªÉ th·ª±c hi·ªán t√¨m ki·∫øm.
# N√≥ t·∫£i si√™u d·ªØ li·ªáu v√† c√°c vector embedding ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc,
# sau ƒë√≥ cung c·∫•p c√°c h√†m ƒë·ªÉ t√¨m ki·∫øm d·ª±a tr√™n vƒÉn b·∫£n ho·∫∑c h√¨nh ·∫£nh.
# --------------------------------------------------

import torch
import clip
import numpy as np
import pandas as pd
from PIL import Image

# --- C·∫•u h√¨nh v√† T·∫£i t√†i nguy√™n ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL, PREPROCESS = clip.load("ViT-B/32", device=DEVICE)

METADATA_PATH = 'db_metadata.csv'
EMBEDDINGS_PATH = 'db_embeddings.npz'

# T·∫£i d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l·∫≠p ch·ªâ m·ª•c
try:
    df_metadata = pd.read_csv(METADATA_PATH)
    embeddings_data = np.load(EMBEDDINGS_PATH)
    text_embeddings_db = embeddings_data['text']
    image_embeddings_db = embeddings_data['image']
    print("Backend: ƒê√£ t·∫£i th√†nh c√¥ng si√™u d·ªØ li·ªáu v√† embeddings.")
except FileNotFoundError:
    print("L·ªói Backend: Kh√¥ng t√¨m th·∫•y file c∆° s·ªü d·ªØ li·ªáu. Vui l√≤ng ch·∫°y 'ingest_data.py' tr∆∞·ªõc.")
    df_metadata = pd.DataFrame()
    text_embeddings_db, image_embeddings_db = None, None

def search_by_text(query_text, top_k=5):
    """
    Th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ nghƒ©a d·ª±a tr√™n m·ªôt truy v·∫•n vƒÉn b·∫£n.
    ƒê√¢y l√† k·ªãch b·∫£n Truy xu·∫•t VƒÉn b·∫£n-t·ªõi-Video (Text-to-Video).
    """
    if text_embeddings_db is None:
        return

    # 1. M√£ h√≥a truy v·∫•n vƒÉn b·∫£n th√†nh vector
    text_tokens = clip.tokenize([query_text], truncate=True).to(DEVICE)
    with torch.no_grad():
        query_embedding = MODEL.encode_text(text_tokens).cpu().numpy()
    
    # 2. Chu·∫©n h√≥a vector truy v·∫•n
    query_embedding /= np.linalg.norm(query_embedding)

    # 3. T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine
    # (T√≠ch v√¥ h∆∞·ªõng c·ªßa c√°c vector ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a ch√≠nh l√† cosine similarity)
    similarities = (text_embeddings_db @ query_embedding.T).squeeze()

    # 4. L·∫•y ra top_k k·∫øt qu·∫£ c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    # 5. Tr·∫£ v·ªÅ si√™u d·ªØ li·ªáu c·ªßa c√°c k·∫øt qu·∫£
    results = df_metadata.iloc[top_indices].to_dict('records')
    for i, res in enumerate(results):
        res['similarity'] = similarities[top_indices[i]]
        
    return results

def search_by_image(query_image, top_k=5):
    """
    Th·ª±c hi·ªán t√¨m ki·∫øm t∆∞∆°ng t·ª± th·ªã gi√°c d·ª±a tr√™n m·ªôt h√¨nh ·∫£nh ƒë·∫ßu v√†o.
    ƒê√¢y l√† k·ªãch b·∫£n Truy xu·∫•t H√¨nh ·∫£nh-t·ªõi-Video (Image-to-Video).
    """
    if image_embeddings_db is None:
        return

    # 1. X·ª≠ l√Ω v√† m√£ h√≥a h√¨nh ·∫£nh truy v·∫•n
    image = PREPROCESS(query_image).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        query_embedding = MODEL.encode_image(image).cpu().numpy()

    # 2. Chu·∫©n h√≥a vector truy v·∫•n
    query_embedding /= np.linalg.norm(query_embedding)

    # 3. T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine
    similarities = (image_embeddings_db @ query_embedding.T).squeeze()

    # 4. L·∫•y ra top_k k·∫øt qu·∫£
    top_indices = np.argsort(similarities)[::-1][:top_k]

    # 5. Tr·∫£ v·ªÅ si√™u d·ªØ li·ªáu
    results = df_metadata.iloc[top_indices].to_dict('records')
    for i, res in enumerate(results):
        res['similarity'] = similarities[top_indices[i]]
        
    return results

```

```python
# file: app.py
# --------------------------------------------------
# GIAI ƒêO·∫†N 1: MVP - GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG
# --------------------------------------------------
# S·ª≠ d·ª•ng Streamlit ƒë·ªÉ t·∫°o m·ªôt giao di·ªán web ƒë∆°n gi·∫£n cho h·ªá th·ªëng.
# Ng∆∞·ªùi d√πng c√≥ th·ªÉ nh·∫≠p vƒÉn b·∫£n ho·∫∑c t·∫£i l√™n h√¨nh ·∫£nh ƒë·ªÉ t√¨m ki·∫øm.
# Ch·∫°y ·ª©ng d·ª•ng b·∫±ng l·ªánh: streamlit run app.py
# L·∫•y c·∫£m h·ª©ng t·ª´ c√°c d·ª± √°n m√£ ngu·ªìn m·ªü t∆∞∆°ng t·ª±.[2]
# --------------------------------------------------

import streamlit as st
from PIL import Image
import os
import retrieval_backend

# --- C·∫•u h√¨nh Giao di·ªán ---
st.set_page_config(page_title="H·ªá th·ªëng Truy xu·∫•t Video", layout="wide")
st.title("üé¨ H·ªá th·ªëng Truy xu·∫•t Video ƒêa ph∆∞∆°ng th·ª©c")
st.markdown("D·ª±a tr√™n b√°o c√°o nghi√™n c·ª©u v·ªÅ ki·∫øn tr√∫c Hybrid v√† m√¥ h√¨nh Embedding kh√¥ng gian chung.")

KEYFRAME_DIR = 'keyframes/'

def display_results(results):
    """H√†m ti·ªán √≠ch ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£ t√¨m ki·∫øm."""
    if not results:
        st.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ph√π h·ª£p.")
        return

    st.success(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£:")
    
    cols = st.columns(len(results))
    for i, res in enumerate(results):
        with cols[i]:
            st.markdown(f"**{res['title']}**")
            
            keyframe_path = os.path.join(KEYFRAME_DIR, str(res['keyframe']))
            if os.path.exists(keyframe_path):
                st.image(keyframe_path, use_column_width=True)
            else:
                st.error(f"Kh√¥ng t√¨m th·∫•y keyframe: {res['keyframe']}")
            
            st.markdown(f"**T√≥m t·∫Øt:** {res['summary']}")
            st.info(f"Th·ªùi gian: {res['start_time']} - {res['end_time']}")
            st.progress(res['similarity'])
            st.caption(f"ƒê·ªô t∆∞∆°ng ƒë·ªìng: {res['similarity']:.4f}")


# --- T·∫°o c√°c Tab cho t·ª´ng lo·∫°i truy v·∫•n ---
tab1, tab2 = st.tabs()

with tab1:
    st.header("T√¨m ki·∫øm Video b·∫±ng M√¥ t·∫£ VƒÉn b·∫£n")
    text_query = st.text_input(
        "Nh·∫≠p m√¥ t·∫£ c·ªßa b·∫°n:", 
        placeholder="v√≠ d·ª•: m·ªôt ch∆∞∆°ng tr√¨nh n√≥i v·ªÅ gi·∫£i b√≥ng ch√†y chuy√™n nghi·ªáp"
    )
    if st.button("T√¨m ki·∫øm", key="text_search_btn"):
        if text_query:
            with st.spinner("ƒêang t√¨m ki·∫øm..."):
                results = retrieval_backend.search_by_text(text_query)
                display_results(results)
        else:
            st.error("Vui l√≤ng nh·∫≠p m√¥ t·∫£ ƒë·ªÉ t√¨m ki·∫øm.")

with tab2:
    st.header("T√¨m ki·∫øm Video b·∫±ng H√¨nh ·∫£nh T∆∞∆°ng t·ª±")
    uploaded_file = st.file_uploader(
        "T·∫£i l√™n m·ªôt h√¨nh ·∫£nh ƒë·ªÉ t√¨m c√°c video c√≥ c·∫£nh quay t∆∞∆°ng t·ª±:", 
        type=["jpg", "jpeg", "png"]
    )
    if uploaded_file is not None:
        query_image = Image.open(uploaded_file)
        st.image(query_image, caption="H√¨nh ·∫£nh truy v·∫•n", width=200)
        
        if st.button("T√¨m ki·∫øm", key="image_search_btn"):
            with st.spinner("ƒêang t√¨m ki·∫øm..."):
                results = retrieval_backend.search_by_image(query_image)
                display_results(results)

```

```python
# file: advanced_concepts.py
# --------------------------------------------------
# GIAI ƒêO·∫†N 2 & 3: C√ÅC KH√ÅI NI·ªÜM N√ÇNG CAO (M√É KH√ÅI NI·ªÜM)
# --------------------------------------------------
# Ph·∫ßn n√†y ch·ª©a m√£ gi·∫£ v√† c√°c ƒëo·∫°n m√£ kh√°i ni·ªám ƒë·ªÉ minh h·ªça c√°ch tri·ªÉn khai
# c√°c k·ªπ thu·∫≠t n√¢ng cao ƒë√£ ƒë∆∞·ª£c th·∫£o lu·∫≠n trong b√°o c√°o nghi√™n c·ª©u.
# ƒê√¢y kh√¥ng ph·∫£i l√† m√£ c√≥ th·ªÉ ch·∫°y tr·ª±c ti·∫øp m√† l√† b·∫£n thi·∫øt k·∫ø ƒë·ªÉ ph√°t tri·ªÉn th√™m.
# --------------------------------------------------

# --- 1. Truy xu·∫•t K·∫øt h·ª£p (Hybrid Search) v·ªõi Elasticsearch v√† Milvus ---
# Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ c√°c client cho Elasticsearch v√† Milvus
# from elasticsearch import Elasticsearch
# from pymilvus import MilvusClient

# es_client = Elasticsearch("http://localhost:9200")
# milvus_client = MilvusClient(uri="http://localhost:19530")

def hybrid_search(text_query, ner_filters, top_k=10):
    """
    Minh h·ªça quy tr√¨nh truy xu·∫•t k·∫øt h·ª£p ƒëa giai ƒëo·∫°n.
    """
    # Giai ƒëo·∫°n 1: L·ªçc ch√≠nh x√°c b·∫±ng Elasticsearch [3]
    # S·ª≠ d·ª•ng c√°c tr∆∞·ªùng `ner` ƒë·ªÉ thu h·∫πp kh√¥ng gian t√¨m ki·∫øm.
    es_query = {
        "query": {
            "bool": {
                "must": [{"match": {"super_document": text_query}}],
                "filter": [{"term": {f"ner.{k}": v}} for k, v in ner_filters.items()]
            }
        },
        "size": 1000 # L·∫•y m·ªôt t·∫≠p ·ª©ng vi√™n l·ªõn
    }
    es_results = es_client.search(index="videos", body=es_query)
    candidate_ids = [doc['_id'] for doc in es_results['hits']['hits']]

    if not candidate_ids:
        return

    # Giai ƒëo·∫°n 2: T√¨m ki·∫øm ng·ªØ nghƒ©a tr√™n t·∫≠p ƒë√£ l·ªçc v·ªõi Milvus [4]
    # M√£ h√≥a truy v·∫•n vƒÉn b·∫£n
    # query_vector = model.encode_text(text_query)
    query_vector = [np.random.rand(512).tolist()] # Vector gi·∫£

    # Th·ª±c hi·ªán t√¨m ki·∫øm ANN ch·ªâ tr√™n c√°c ID ·ª©ng vi√™n
    milvus_results = milvus_client.search(
        collection_name="videos_collection",
        data=query_vector,
        limit=top_k,
        expr=f"video_id in {candidate_ids}" # L·ªçc tr∆∞·ªõc khi t√¨m ki·∫øm
    )
    
    # Giai ƒëo·∫°n 3: H·ª£p nh·∫•t v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ cu·ªëi c√πng
    # (Logic ƒë·ªÉ k·∫øt h·ª£p ƒëi·ªÉm s·ªë t·ª´ ES v√† Milvus, v√≠ d·ª•: RRF [5])
    final_results = milvus_results
    return final_results


# --- 2. M√¥ h√¨nh h√≥a Th·ªùi gian (Temporal Modeling) ---
# import torch.nn as nn

# class TemporalModeler(nn.Module):
#     def __init__(self, embedding_dim, num_heads, num_layers):
#         super().__init__()
#         encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)
#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

#     def forward(self, frame_embeddings):
#         # frame_embeddings c√≥ shape: (num_frames, batch_size, embedding_dim)
#         temporal_output = self.transformer_encoder(frame_embeddings)
#         return temporal_output

# # So s√°nh c√°ch ti·∫øp c·∫≠n
# def get_video_embedding(frames):
#     # frames: danh s√°ch c√°c ·∫£nh PIL
#     # image_encoder: b·ªô m√£ h√≥a h√¨nh ·∫£nh c·ªßa CLIP
#     frame_embeddings = image_encoder(frames) # Shape: [num_frames, embedding_dim]

#     # C√°ch ti·∫øp c·∫≠n Mean Pooling (C∆° b·∫£n) [6]
#     video_embedding_simple = frame_embeddings.mean(dim=0)

#     # C√°ch ti·∫øp c·∫≠n Temporal Transformer (N√¢ng cao) [7]
#     temporal_model = TemporalModeler(embedding_dim=512, num_heads=8, num_layers=4)
#     # C·∫ßn thay ƒë·ªïi shape ƒë·ªÉ ph√π h·ª£p v·ªõi Transformer
#     temporal_input = frame_embeddings.unsqueeze(1) # -> [num_frames, 1, embedding_dim]
#     temporal_output = temporal_model(temporal_input)
#     video_embedding_advanced = temporal_output.mean(dim=0).squeeze(0)
    
#     return video_embedding_simple, video_embedding_advanced


# --- 3. T√≠ch h·ª£p √Çm thanh v·ªõi TEFAL (Text-Conditioned Feature Alignment) ---
# ƒê√¢y l√† m√£ kh√°i ni·ªám minh h·ªça ki·∫øn tr√∫c TEFAL.[8, 9]

# class CrossAttentionBlock(nn.Module):
#     def __init__(self, embed_dim, num_heads):
#         super().__init__()
#         self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)

#     def forward(self, query, key_value):
#         # query: embedding vƒÉn b·∫£n (batch, 1, dim)
#         # key_value: embedding √¢m thanh/video (batch, seq_len, dim)
#         attn_output, _ = self.multihead_attn(query, key_value, key_value)
#         return attn_output

# class TEFAL_Model(nn.Module):
#     def __init__(self, embed_dim=512, num_heads=8):
#         super().__init__()
#         # C√°c b·ªô m√£ h√≥a cho t·ª´ng ph∆∞∆°ng th·ª©c
#         self.text_encoder =... # CLIP text encoder
#         self.video_encoder =... # CLIP image encoder (cho t·ª´ng frame)
#         self.audio_encoder =... # AST model

#         # C√°c kh·ªëi cross-attention ƒë·ªôc l·∫≠p
#         self.text_video_aligner = CrossAttentionBlock(embed_dim, num_heads)
#         self.text_audio_aligner = CrossAttentionBlock(embed_dim, num_heads)

#     def forward(self, text, video_frames, audio_spectrogram):
#         # 1. Tr√≠ch xu·∫•t embedding th√¥
#         text_emb = self.text_encoder(text).unsqueeze(1) # (batch, 1, dim)
#         video_embs = self.video_encoder(video_frames)   # (batch, num_frames, dim)
#         audio_embs = self.audio_encoder(audio_spectrogram) # (batch, num_audio_patches, dim)

#         # 2. ƒêi·ªÅu ch·ªânh ƒë·∫∑c tr∆∞ng video v√† √¢m thanh b·∫±ng vƒÉn b·∫£n
#         conditioned_video_emb = self.text_video_aligner(query=text_emb, key_value=video_embs)
#         conditioned_audio_emb = self.text_audio_aligner(query=text_emb, key_value=audio_embs)

#         # 3. H·ª£p nh·∫•t c√°c embedding ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh
#         # TEFAL ƒë·ªÅ xu·∫•t h·ª£p nh·∫•t b·∫±ng ph√©p c·ªông ƒë∆°n gi·∫£n
#         final_embedding = conditioned_video_emb + conditioned_audio_emb
        
#         return final_embedding.squeeze(1)

